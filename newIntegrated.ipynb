{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/Library/Python/3.12/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load intents from intent.json\n",
    "with open('intent.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "# Initialize the tokenizer and model (use a pre-trained BERT model fine-tuned for classification)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your fine-tuned model path if needed\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')  # Or path to your fine-tuned model\n",
    "\n",
    "def get_intent(user_input):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference with the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits and predict the class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Map predicted class to intent\n",
    "    return intents['intents'][predicted_class]['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant item: Rava Kichadi\n",
      "Description: A healthy breakfast option made with semolina and vegetables lightly tempered with spices cooked to perfection\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load the menu data (assuming it's stored in a 'mockMenu 1.json' file)\n",
    "with open('mockMenu 1.json', 'r') as f:\n",
    "    menu_data = json.load(f)[\"menu\"]\n",
    "\n",
    "# Preprocess the menu items (convert descriptions to list of tokens for BM25)\n",
    "# Filter out menu items that have a 'None' description\n",
    "menu_texts = [item['description'] for item in menu_data if item['description'] is not None]\n",
    "menu_tokens = [text.split() for text in menu_texts]  # Tokenization of the valid menu descriptions\n",
    "\n",
    "# Initialize BM25 model\n",
    "bm25 = BM25Okapi(menu_tokens)\n",
    "\n",
    "# Function to retrieve the most relevant menu item\n",
    "def retrieve_menu_items(query):\n",
    "    query_tokens = query.split()  # Tokenizing the user's query\n",
    "    scores = bm25.get_scores(query_tokens)  # BM25 scores for each menu item based on the query\n",
    "    best_idx = scores.argmax()  # Index of the highest-scoring menu item\n",
    "    return menu_data[best_idx]  # Return the most relevant menu item\n",
    "\n",
    "# Example usage\n",
    "query = \"healthy breakfast option\"\n",
    "result = retrieve_menu_items(query)\n",
    "print(f\"Most relevant item: {result['itemName']}\\nDescription: {result['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/Library/Python/3.12/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'DistilBertForMaskedLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize DistilBERT for response generation\n",
    "generator = pipeline(\"text-generation\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "def generate_response(intent, retrieved_item):\n",
    "    if intent == \"quickest_dish\":\n",
    "        prompt = f\"The quickest dish is {retrieved_item['itemName']} which takes {retrieved_item['prepTimeInMins']} minutes to prepare.\"\n",
    "    elif intent == \"chef_special\":\n",
    "        prompt = f\"Today's special is {retrieved_item['itemName']}. It is {retrieved_item['description']}.\"\n",
    "    elif intent == \"diet_recommendation\":\n",
    "        prompt = f\"Based on your condition, I recommend {retrieved_item['itemName']}. {retrieved_item['description']}.\"\n",
    "    elif intent == \"price_inquiry\":\n",
    "        prompt = f\"The price of {retrieved_item['itemName']} is ${retrieved_item['price']}.\"\n",
    "    else:\n",
    "        prompt = f\"{retrieved_item['itemName']} is a great choice!\"\n",
    "\n",
    "    return generator(prompt, max_length=50)[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize distilGPT-2 for response generation\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "def generate_response(intent, retrieved_item):\n",
    "    if intent == \"quickest_dish\":\n",
    "        prompt = f\"The quickest dish is {retrieved_item['itemName']} which takes {retrieved_item['prepTimeInMins']} minutes to prepare.\"\n",
    "    elif intent == \"chef_special\":\n",
    "        prompt = f\"Today's special is {retrieved_item['itemName']}. It is {retrieved_item['description']}.\"\n",
    "    elif intent == \"diet_recommendation\":\n",
    "        prompt = f\"Based on your condition, I recommend {retrieved_item['itemName']}. {retrieved_item['description']}.\"\n",
    "    elif intent == \"price_inquiry\":\n",
    "        prompt = f\"The price of {retrieved_item['itemName']} is ${retrieved_item['price']}.\"\n",
    "    else:\n",
    "        prompt = f\"{retrieved_item['itemName']} is a great choice!\"\n",
    "\n",
    "    # Generate response using distilGPT2\n",
    "    generated = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input):\n",
    "    # Step 1: Intent Recognition using BERT\n",
    "    intent = get_intent(user_input)\n",
    "    \n",
    "    # Step 2: Document Retrieval using BM25\n",
    "    retrieved_item = retrieve_menu_items(user_input)\n",
    "    \n",
    "    # Step 3: Response Generation using DistilBERT\n",
    "    response = generate_response(intent, retrieved_item)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baingan Ka Bharta is a great choice!\n",
      "This book takes you on an astonishing journey through China, through India, through Australia, through Asia and beyond! I’ll tell you more.\n",
      "This book is as close to as\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "user_input = \"What is the quickest dish on your menu?\"\n",
    "response = chat(user_input)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
