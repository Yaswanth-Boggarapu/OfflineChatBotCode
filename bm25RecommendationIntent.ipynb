{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ju82TdjW-lC",
        "outputId": "38f4b822-e661-4c1b-82dc-97fa21dfc413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK data for tokenization, stopwords, and lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Sample menu data (same as before)\n",
        "menu_data = [\n",
        "    {\n",
        "        \"itemId\": \"1\",\n",
        "        \"itemName\": \"Chicken Curry\",\n",
        "        \"description\": \"Hot and spicy chicken dish\",\n",
        "        \"specialInstructions\": \"Extra spicy\",\n",
        "        \"allergicInfo\": \"Contains nuts\",\n",
        "        \"price\": \"15\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"2\",\n",
        "        \"itemName\": \"Mild Chicken Pasta\",\n",
        "        \"description\": \"Creamy pasta with chicken\",\n",
        "        \"specialInstructions\": \"No garlic\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"12\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"3\",\n",
        "        \"itemName\": \"Spicy Pasta\",\n",
        "        \"description\": \"Pasta with a spicy tomato sauce\",\n",
        "        \"specialInstructions\": \"\",\n",
        "        \"allergicInfo\": \"\",\n",
        "        \"price\": \"10\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"4\",\n",
        "        \"itemName\": \"Beef Stroganoff\",\n",
        "        \"description\": \"Creamy beef with mushrooms and onions\",\n",
        "        \"specialInstructions\": \"Less sauce\",\n",
        "        \"allergicInfo\": \"Dairy, Mushrooms\",\n",
        "        \"price\": \"18\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"5\",\n",
        "        \"itemName\": \"Vegetarian Pizza\",\n",
        "        \"description\": \"Wood-fired pizza topped with vegetables\",\n",
        "        \"specialInstructions\": \"Extra cheese\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"14\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"6\",\n",
        "        \"itemName\": \"Grilled Salmon\",\n",
        "        \"description\": \"Freshly grilled salmon with herbs\",\n",
        "        \"specialInstructions\": \"No oil\",\n",
        "        \"allergicInfo\": \"Fish\",\n",
        "        \"price\": \"20\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"7\",\n",
        "        \"itemName\": \"Caesar Salad\",\n",
        "        \"description\": \"Crisp romaine lettuce with Caesar dressing\",\n",
        "        \"specialInstructions\": \"No croutons\",\n",
        "        \"allergicInfo\": \"Dairy, Gluten, Anchovies\",\n",
        "        \"price\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"8\",\n",
        "        \"itemName\": \"Chocolate Lava Cake\",\n",
        "        \"description\": \"Rich chocolate cake with molten center\",\n",
        "        \"specialInstructions\": \"No whipped cream\",\n",
        "        \"allergicInfo\": \"Dairy, Eggs\",\n",
        "        \"price\": \"7\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"9\",\n",
        "        \"itemName\": \"Lamb Biryani\",\n",
        "        \"description\": \"Fragrant rice with tender lamb and spices\",\n",
        "        \"specialInstructions\": \"No cilantro\",\n",
        "        \"allergicInfo\": \"Contains nuts\",\n",
        "        \"price\": \"17\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"10\",\n",
        "        \"itemName\": \"Tofu Stir-Fry\",\n",
        "        \"description\": \"Stir-fried tofu with mixed vegetables\",\n",
        "        \"specialInstructions\": \"No soy sauce\",\n",
        "        \"allergicInfo\": \"Soy\",\n",
        "        \"price\": \"13\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"11\",\n",
        "        \"itemName\": \"Garlic Bread\",\n",
        "        \"description\": \"Crispy garlic bread slices\",\n",
        "        \"specialInstructions\": \"No butter\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"12\",\n",
        "        \"itemName\": \"Fruit Salad\",\n",
        "        \"description\": \"Fresh seasonal fruits\",\n",
        "        \"specialInstructions\": \"No bananas\",\n",
        "        \"allergicInfo\": \"\",\n",
        "        \"price\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"13\",\n",
        "        \"itemName\": \"Butter Chicken\",\n",
        "        \"description\": \"Chicken cooked in a creamy tomato sauce\",\n",
        "        \"specialInstructions\": \"Extra gravy\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"16\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"14\",\n",
        "        \"itemName\": \"Margherita Pizza\",\n",
        "        \"description\": \"Classic pizza with mozzarella and basil\",\n",
        "        \"specialInstructions\": \"Add olives\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"12\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"15\",\n",
        "        \"itemName\": \"Greek Salad\",\n",
        "        \"description\": \"Salad with cucumbers, tomatoes, and feta cheese\",\n",
        "        \"specialInstructions\": \"No onions\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"9\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Step 1: Tokenize and preprocess the document (menu items)\n",
        "def tokenize_and_process(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Step 2: Preprocess menu data into a list of combined item details\n",
        "def preprocess_menu(menu_data):\n",
        "    return [\n",
        "        f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')} {item.get('price', '')}\"\n",
        "        for item in menu_data\n",
        "    ]\n",
        "\n",
        "# Step 3: Build an inverted index\n",
        "def build_inverted_index(menu_data):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, item in enumerate(menu_data):\n",
        "        text = f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')}\"\n",
        "        tokens = tokenize_and_process(text)\n",
        "        for token in set(tokens):  # Use set to avoid duplicate entries for the same word\n",
        "            inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "# Step 4: Retrieve relevant documents using inverted index\n",
        "def retrieve_documents(query, inverted_index):\n",
        "    query_tokens = tokenize_and_process(query)\n",
        "    relevant_docs = set()\n",
        "\n",
        "    # Collect documents that contain at least one query term\n",
        "    for token in query_tokens:\n",
        "        if token in inverted_index:\n",
        "            relevant_docs.update(inverted_index[token])\n",
        "\n",
        "    return list(relevant_docs)\n",
        "\n",
        "# Step 5: Rank the retrieved documents using BM25\n",
        "def rank_documents(query, menu_data, relevant_doc_ids):\n",
        "    preprocessed_menu = preprocess_menu(menu_data)\n",
        "    tokenized_menu = [tokenize_and_process(doc) for doc in preprocessed_menu]\n",
        "    bm25 = BM25Okapi(tokenized_menu)\n",
        "    tokenized_query = tokenize_and_process(query)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    relevant_scores = [(doc_id, scores[doc_id]) for doc_id in relevant_doc_ids]\n",
        "    ranked_docs = sorted(relevant_scores, key=lambda x: x[1], reverse=True)\n",
        "    return ranked_docs\n",
        "\n",
        "# Step 6: Build an inverted index from the ranked documents\n",
        "def build_inverted_index_from_ranked_docs(ranked_docs, menu_data):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, score in ranked_docs:\n",
        "        item = menu_data[doc_id]\n",
        "        text = f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')}\"\n",
        "        tokens = tokenize_and_process(text)\n",
        "        for token in set(tokens):\n",
        "            inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "# Step 7: Define and detect intent based on query patterns\n",
        "def detect_intent(query):\n",
        "    query = query.lower()\n",
        "\n",
        "    # Basic intent detection patterns\n",
        "    if re.search(r\"\\bprice\\b\", query):\n",
        "        return \"price\"\n",
        "    elif re.search(r\"\\ballergi(?:es|c|info)\\b\", query):\n",
        "        return \"allergic_info\"\n",
        "    elif re.search(r\"\\binstruction\\b|\\bpreparation\\b\", query):\n",
        "        return \"special_instructions\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "# Step 8: Generate a response based on detected intent and retrieved documents\n",
        "def generate_response(query, ranked_docs, menu_data):\n",
        "    intent = detect_intent(query)\n",
        "\n",
        "    if not ranked_docs:\n",
        "        return \"Sorry, we couldn't find any matching items for your query.\"\n",
        "\n",
        "    response = \"\"\n",
        "    for doc_id, score in ranked_docs:\n",
        "        item = menu_data[doc_id]\n",
        "\n",
        "        if intent == \"price\":\n",
        "            response += f\"Item: {item['itemName']} | Price: ${item['price']}\\n\"\n",
        "        elif intent == \"allergic_info\":\n",
        "            response += f\"Item: {item['itemName']} | Allergic Info: {item.get('allergicInfo', 'No specific allergic info')}\\n\"\n",
        "        elif intent == \"special_instructions\":\n",
        "            response += f\"Item: {item['itemName']} | Special Instructions: {item.get('specialInstructions', 'None')}\\n\"\n",
        "        else:\n",
        "            response += f\"Item: {item['itemName']} | Description: {item['description']}\\n\"\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "# Build the inverted index\n",
        "inverted_index = build_inverted_index(menu_data)\n",
        "\n",
        "# Example query\n",
        "query = \"Fever Recommendations?\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "relevant_docs = retrieve_documents(query, inverted_index)\n",
        "print(f\"Relevant document IDs for query '{query}':\", relevant_docs)\n",
        "\n",
        "# Rank the retrieved documents using BM25\n",
        "ranked_docs = rank_documents(query, menu_data, relevant_docs)\n",
        "print(f\"Ranked document IDs and scores for query '{query}':\", ranked_docs)\n",
        "\n",
        "# Build the inverted index from ranked documents\n",
        "ranked_inverted_index = build_inverted_index_from_ranked_docs(ranked_docs, menu_data)\n",
        "\n",
        "# Generate a response based on the ranked documents and detected intent\n",
        "response = generate_response(query, ranked_docs, menu_data)\n",
        "print(\"//////////////////////////\")\n",
        "print(\"\\nResponse based on detected intent:\")\n",
        "print(response)\n",
        "print(\"//////////////////////////\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0DS-5_TXC4b",
        "outputId": "7f5907cb-2fb0-48c6-ca93-b965bd916f3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant document IDs for query 'Fever Recommendations?': []\n",
            "Ranked document IDs and scores for query 'Fever Recommendations?': []\n",
            "//////////////////////////\n",
            "\n",
            "Response based on detected intent:\n",
            "Sorry, we couldn't find any matching items for your query.\n",
            "//////////////////////////\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK data for tokenization, stopwords, and lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Sample menu data (same as before)\n",
        "menu_data = [\n",
        "    {\n",
        "        \"itemId\": \"1\",\n",
        "        \"itemName\": \"Chicken Curry\",\n",
        "        \"description\": \"Hot and spicy chicken dish\",\n",
        "        \"specialInstructions\": \"Extra spicy\",\n",
        "        \"allergicInfo\": \"Contains nuts\",\n",
        "        \"price\": \"15\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"2\",\n",
        "        \"itemName\": \"Mild Chicken Pasta\",\n",
        "        \"description\": \"Creamy pasta with chicken\",\n",
        "        \"specialInstructions\": \"No garlic\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"12\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"3\",\n",
        "        \"itemName\": \"Spicy Pasta\",\n",
        "        \"description\": \"Pasta with a spicy tomato sauce\",\n",
        "        \"specialInstructions\": \"\",\n",
        "        \"allergicInfo\": \"\",\n",
        "        \"price\": \"10\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"4\",\n",
        "        \"itemName\": \"Beef Stroganoff\",\n",
        "        \"description\": \"Creamy beef with mushrooms and onions\",\n",
        "        \"specialInstructions\": \"Less sauce\",\n",
        "        \"allergicInfo\": \"Dairy, Mushrooms\",\n",
        "        \"price\": \"18\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"5\",\n",
        "        \"itemName\": \"Vegetarian Pizza\",\n",
        "        \"description\": \"Wood-fired pizza topped with vegetables\",\n",
        "        \"specialInstructions\": \"Extra cheese\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"14\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"6\",\n",
        "        \"itemName\": \"Grilled Salmon\",\n",
        "        \"description\": \"Freshly grilled salmon with herbs\",\n",
        "        \"specialInstructions\": \"No oil\",\n",
        "        \"allergicInfo\": \"Fish\",\n",
        "        \"price\": \"20\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"7\",\n",
        "        \"itemName\": \"Caesar Salad\",\n",
        "        \"description\": \"Crisp romaine lettuce with Caesar dressing\",\n",
        "        \"specialInstructions\": \"No croutons\",\n",
        "        \"allergicInfo\": \"Dairy, Gluten, Anchovies\",\n",
        "        \"price\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"8\",\n",
        "        \"itemName\": \"Chocolate Lava Cake\",\n",
        "        \"description\": \"Rich chocolate cake with molten center\",\n",
        "        \"specialInstructions\": \"No whipped cream\",\n",
        "        \"allergicInfo\": \"Dairy, Eggs\",\n",
        "        \"price\": \"7\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"9\",\n",
        "        \"itemName\": \"Lamb Biryani\",\n",
        "        \"description\": \"Fragrant rice with tender lamb and spices\",\n",
        "        \"specialInstructions\": \"No cilantro\",\n",
        "        \"allergicInfo\": \"Contains nuts\",\n",
        "        \"price\": \"17\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"10\",\n",
        "        \"itemName\": \"Tofu Stir-Fry\",\n",
        "        \"description\": \"Stir-fried tofu with mixed vegetables\",\n",
        "        \"specialInstructions\": \"No soy sauce\",\n",
        "        \"allergicInfo\": \"Soy\",\n",
        "        \"price\": \"13\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"11\",\n",
        "        \"itemName\": \"Garlic Bread\",\n",
        "        \"description\": \"Crispy garlic bread slices\",\n",
        "        \"specialInstructions\": \"No butter\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"12\",\n",
        "        \"itemName\": \"Fruit Salad\",\n",
        "        \"description\": \"Fresh seasonal fruits\",\n",
        "        \"specialInstructions\": \"No bananas\",\n",
        "        \"allergicInfo\": \"\",\n",
        "        \"price\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"13\",\n",
        "        \"itemName\": \"Butter Chicken\",\n",
        "        \"description\": \"Chicken cooked in a creamy tomato sauce\",\n",
        "        \"specialInstructions\": \"Extra gravy\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"16\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"14\",\n",
        "        \"itemName\": \"Margherita Pizza\",\n",
        "        \"description\": \"Classic pizza with mozzarella and basil\",\n",
        "        \"specialInstructions\": \"Add olives\",\n",
        "        \"allergicInfo\": \"Gluten, Dairy\",\n",
        "        \"price\": \"12\"\n",
        "    },\n",
        "    {\n",
        "        \"itemId\": \"15\",\n",
        "        \"itemName\": \"Greek Salad\",\n",
        "        \"description\": \"Salad with cucumbers, tomatoes, and feta cheese\",\n",
        "        \"specialInstructions\": \"No onions\",\n",
        "        \"allergicInfo\": \"Dairy\",\n",
        "        \"price\": \"9\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Step 1: Tokenize and preprocess the document (menu items)\n",
        "def tokenize_and_process(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Step 2: Preprocess menu data into a list of combined item details\n",
        "def preprocess_menu(menu_data):\n",
        "    return [\n",
        "        f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')} {item.get('price', '')}\"\n",
        "        for item in menu_data\n",
        "    ]\n",
        "\n",
        "# Step 3: Build an inverted index\n",
        "def build_inverted_index(menu_data):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, item in enumerate(menu_data):\n",
        "        text = f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')}\"\n",
        "        tokens = tokenize_and_process(text)\n",
        "        for token in set(tokens):  # Use set to avoid duplicate entries for the same word\n",
        "            inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "# Step 4: Retrieve relevant documents using inverted index\n",
        "def retrieve_documents(query, inverted_index):\n",
        "    query_tokens = tokenize_and_process(query)\n",
        "    relevant_docs = set()\n",
        "\n",
        "    # Collect documents that contain at least one query term\n",
        "    for token in query_tokens:\n",
        "        if token in inverted_index:\n",
        "            relevant_docs.update(inverted_index[token])\n",
        "\n",
        "    return list(relevant_docs)\n",
        "\n",
        "# Step 5: Rank the retrieved documents using BM25\n",
        "def rank_documents(query, menu_data, relevant_doc_ids):\n",
        "    preprocessed_menu = preprocess_menu(menu_data)\n",
        "    tokenized_menu = [tokenize_and_process(doc) for doc in preprocessed_menu]\n",
        "    bm25 = BM25Okapi(tokenized_menu)\n",
        "    tokenized_query = tokenize_and_process(query)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    relevant_scores = [(doc_id, scores[doc_id]) for doc_id in relevant_doc_ids]\n",
        "    ranked_docs = sorted(relevant_scores, key=lambda x: x[1], reverse=True)\n",
        "    return ranked_docs\n",
        "\n",
        "# Step 6: Build an inverted index from the ranked documents\n",
        "def build_inverted_index_from_ranked_docs(ranked_docs, menu_data):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, score in ranked_docs:\n",
        "        item = menu_data[doc_id]\n",
        "        text = f\"{item['itemName']} {item['description']} {item.get('specialInstructions', '')} {item.get('allergicInfo', '')}\"\n",
        "        tokens = tokenize_and_process(text)\n",
        "        for token in set(tokens):\n",
        "            inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "# Step 7: Define and detect intent based on query patterns\n",
        "def detect_intent(query):\n",
        "    query = query.lower()\n",
        "\n",
        "    # Basic intent detection patterns\n",
        "    if re.search(r\"\\bfever\\b\", query):\n",
        "        return \"fever_recommendation\"\n",
        "    elif re.search(r\"\\bspicy\\b\", query):\n",
        "        return \"spicy_food\"\n",
        "    elif re.search(r\"\\bprice\\b\", query):\n",
        "        return \"price\"\n",
        "    elif re.search(r\"\\ballergi(?:es|c|info)\\b\", query):\n",
        "        return \"allergic_info\"\n",
        "    elif re.search(r\"\\binstruction\\b|\\bpreparation\\b\", query):\n",
        "        return \"special_instructions\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "# Step 8: Recommend menu items based on query intent\n",
        "def recommend_based_on_intent(intent, menu_data):\n",
        "    recommendations = []\n",
        "\n",
        "    if intent == \"fever_recommendation\":\n",
        "        # Recommend non-spicy food for fever\n",
        "        recommendations = [item for item in menu_data if \"spicy\" in item['description'].lower()]\n",
        "    elif intent == \"spicy_food\":\n",
        "        # Return spicy food items\n",
        "        recommendations = [item for item in menu_data if \"spicy\" in item['description'].lower()]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Step 9: Generate a response based on detected intent and retrieved documents\n",
        "# Modified: Generate a response based on detected intent and apply rule-based filtering for fever\n",
        "def generate_response(query, ranked_docs, menu_data):\n",
        "    intent = detect_intent(query)\n",
        "\n",
        "    # Special case: If intent is for a fever recommendation, filter the recommendations\n",
        "    if intent == \"fever_recommendation\":\n",
        "        recommendations = recommend_based_on_intent(intent, menu_data)\n",
        "        if recommendations:\n",
        "            response = \"Here are some recommended items for fever (non-spicy):\\n\"\n",
        "            for item in recommendations:\n",
        "                response += f\"Item: {item['itemName']} | Description: {item['description']}\\n\"\n",
        "            return response\n",
        "        else:\n",
        "            return \"Sorry, no non-spicy items found in the menu.\"\n",
        "\n",
        "    # Default fallback: Use BM25 ranked results\n",
        "    if not ranked_docs:\n",
        "        return \"Sorry, we couldn't find any matching items for your query.\"\n",
        "\n",
        "    # Otherwise, return the items based on BM25 ranking and other intents\n",
        "    response = \"\"\n",
        "    for doc_id, score in ranked_docs:\n",
        "        item = menu_data[doc_id]\n",
        "\n",
        "        if intent == \"price\":\n",
        "            response += f\"Item: {item['itemName']} | Price: ${item['price']}\\n\"\n",
        "        elif intent == \"allergic_info\":\n",
        "            response += f\"Item: {item['itemName']} | Allergic Info: {item.get('allergicInfo', 'No specific allergic info')}\\n\"\n",
        "        elif intent == \"special_instructions\":\n",
        "            response += f\"Item: {item['itemName']} | Special Instructions: {item.get('specialInstructions', 'None')}\\n\"\n",
        "        else:\n",
        "            response += f\"Item: {item['itemName']} | Description: {item['description']}\\n\"\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "# Step 8: Recommend menu items based on query intent (updated to ensure non-spicy for fever)\n",
        "def recommend_based_on_intent(intent, menu_data):\n",
        "    recommendations = []\n",
        "\n",
        "    if intent == \"fever_recommendation\":\n",
        "        # Recommend non-spicy food for fever\n",
        "        recommendations = [item for item in menu_data if \"spicy\" in item['description'].lower()]\n",
        "    elif intent == \"spicy_food\":\n",
        "        # Return spicy food items\n",
        "        recommendations = [item for item in menu_data if \"spicy\" in item['description'].lower()]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Example query\n",
        "query = \"Can you recommend something for fever?\"\n",
        "\n",
        "# Detect intent and filter based on fever recommendation directly\n",
        "intent = detect_intent(query)\n",
        "response = generate_response(query, [], menu_data)  # Ranked docs are skipped for fever intent\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-34LPMklahcH",
        "outputId": "33ac981b-3d5d-46c0-a668-bf6bbbe9edf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some recommended items for fever (non-spicy):\n",
            "Item: Chicken Curry | Description: Hot and spicy chicken dish\n",
            "Item: Spicy Pasta | Description: Pasta with a spicy tomato sauce\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}